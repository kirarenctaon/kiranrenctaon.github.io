---
title: "PTAP"
bg: orange
color: black
fa-icon: fa fa-codepen

---

## Prototypes of Temporal Activated Patterns (PTAPs)

-------------------------

### Visualziation
{: .left}
![ex_screenshot](./img/visualize.PNG)

We visualize PTAPs with colored lines; the black line is the input sequence, the colored dotted line is the input receptive field, the colored line is the PTAP outcome and the colored shadow denotes the variation of the pattern. Besides, the input receptive fields of the channel activations can have the overlapped areas, so some parts of patterns detected in receptive fields can also be overlapped with other patterns. We remove some overlapped patterns for PTAP to illustrate the important patterns more clearly.

### Improved Prototypes Selection Method
{: .left}
Originally, the greedy prototype selection method based on MMD finds prototypes with the radial basis function kernel (rbf kernel) in the input space. We need monotone submodularity to guarantee that the greedy solution is close to the optimal one. The rbf kernel with a large value of gamma satisfies the conditions for monotone submodularity. However, a large value of gamma makes the algorithm vulnerable to the temporal shift of samples, which hinders distinguishing true shapes of sub-sequences. Furthermore, input values may not reflect the characteristics of internal patterns enough. Our algorithm utilizes the internal feature vectors in the neural network to construct the Gram Kernel matrix. This new kernel matrix imporves the quality of selected prototypes from temporal data. 


### Value-LRP
{: .left}
To understand the relationships among the activations of channels and the extracted prototype patterns, we compute the distribution of the input attributions with respect to each channel. In other words, our goal is to determine which changes in the pattern shape affect more or less the channel's activation. We use Layer-wise Relevance Propagation (LRP)  to compute and calculate how much a change in a temporal position affects the channel's activation. It helps to understand which parts in the given prototype the trained network focuses on.
 
