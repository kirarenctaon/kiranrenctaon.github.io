---
title: "PTAP"
bg: orange
color: black
fa-icon: fa fa-codepen

---

# How PTAP works?
{: .text-purple}

-------------------------

### Visualziation
{: .left}
![ex_screenshot](./img/visualize.PNG)

We visualize PTAPs with colored lines; the black line is the input sequence, the colored dotted line is the input receptive field, the colored line is the PTAP outcome and the colored shadow denotes the variation of the pattern. Besides, the input receptive fields of the channel activations can have the overlapped areas, so some parts of patterns detected in receptive fields can also be overlapped with other patterns. We remove some overlapped patterns for PTAP to illustrate the important patterns more clearly.

-------------------------

### Improved Prototypes Selection
{: .left}
Originally, the greedy prototype selection method based on MMD finds prototypes with the radial basis function kernel (rbf kernel) in the input space. The rbf kernel $k_{i,j}=exp(-\gamma{\Vert x_i-x_j \Vert}^2_2)$ with a large value of $\gamma$ satisfies the conditions for monotone submodularity described in Section \ref{sec:greedyps}. However, a large value of $\gamma$ makes the algorithm vulnerable to the temporal shift of samples, which hinders distinguishing true shapes of sub-sequences. Furthermore, input values may not reflect the characteristics of internal patterns enough. 

-------------------------




### Value-LRP
{: .left}
To understand the relationships among the activations of channels and the extracted prototype patterns, we compute the distribution of the input attributions with respect to each channel. In other words, our goal is to determine which changes in the pattern shape affect more or less the channel's activation. We use Layer-wise Relevance Propagation (LRP) \cite{MONTAVON2017211} to compute and calculate how much a change in a temporal position affects the channel's activation.

-------------------------

