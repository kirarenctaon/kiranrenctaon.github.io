---
title: "Intro"
bg: '#FFFFFF'
color: black

---

## Interpretation of Deep Temporal Representations by Selective Visualization of Internally Activated Nodes
![ex_screenshot](./img/main.png)

Deep neural networks have demonstrated competitive performance in classification tasks for sequential data. However, it remains difficult to understand which temporal patterns the internal channels of deep neural networks capture for decision-making in sequential data. To address this issue, we propose a new framework with which to visualize temporal representations learned in deep neural networks without hand-crafted segmentation labels. Given input data, our framework extracts highly activated temporal regions that contribute to activating internal nodes and characterizes such regions by prototype selection method based on Maximum Mean Discrepancy. Representative temporal patterns referred to here as \textit{Prototypes of Temporal Activated Patterns (PTAPs)} provide core examples of sub-sequences in the sequential data for interpretability. We also analyze the role of each channel by Valeu-LRP plots using representative prototypes and the distribution of the input attribution. Input attribution plots give visual information to recognize the shapes focused on by the channel for decision-making.

#### Contributions
{: .left}
- Without hand-crafted segmentation labels, our framework provides representative sub-sequences that activate channels of a convolutional neural network the most.
- We improve an existing greedy prototype selection method by constructing the Gram kernel matrix with feature values in neural networks instead of the simple radial basis function kernel with input values. The prototype results are more reasonable for time-series as our method is less sensitive to the shift of sub-sequences.
- With our value-attribution score plots that show the distribution of input attributions for each prototype group, our framework helps to understand the shapes focused on by the channel for decision-making purposes.

#### Further Info
{: .left}

&#127908;	
 Accepted at [KDD 2021](https://www.kdd.org/kdd2021/)


&#128195; 
[ axXiv Link](https://arxiv.org/abs/2004.12538)
